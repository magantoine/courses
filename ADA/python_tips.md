# Python Help ADA :


### Do a histogram 

You can either use the **seaborn function** 

```python
sns.histplot(data=df, x="column")
```

You can also use the `np.bincount` function. This function takes **an array x** and outputs an array b such that `b[i]` is the number of times i occurs.
So you can do a histplot by doing :

```python
values = np.bincount(x)
bins = np.arange(len(values))
plt.bar(bins, values)
```

But better : 

```python
plt.hist(values, bins=100)
```


### Plot loglog plots 

We need to plot a loglog plot. We first need to compute the bins of a histogram.

```python
n, bins, patches = plt.hist(values, log=True, bins=100, histtype="step")
```

n = height of the bins
bins = edges of the bins

Now we plot the loglog :
```python
plt.loglog(bins[1:], n)
```


TO have a steadier plot you need to use the anti-cumulative histogram i.e. :
```python
n, bins, patches = plt.hist(values, log=True, bins=1000, histtype="step", cumulative=-1)
plt.loglog(bins[1:], n)
```


### Statis tests:

#### Goodness of fit with a normal distribution

You want to check if `values` come from a normal distribution 

```python
from statsmodels.stats import diagnostic

ksstat, pvalue = diagnostic.kstest_normal(values, dist="norm")
```

You can also check if it fits an exponential distribution 
```python
ksstat, pvalue, diagnostic.kstest_normal(values, dist="exp")
```

In this case the NULL HYPOTHESIS is **the values are generated by this distribution**, so if p < 0.05 we reject the null hypothesis.  


#### Correlation between random variables

To test if two random variables have any relation we use the Pearson test.

```python
from scipy import stats

score, pvalue = stats.pearsonr(values1, values2)
```

The NULL HYPOTHESIS is **the two variables are independant** so if p < 0.05 then the test is significant


#### Test the difference in the mean of two random variables

```python
score, pvalue = stats.ttest_ind(values1, values2)
```

NULL HYPOTHESIS **distribution has the same mean**,, if pvalue < 0.05 we reect the null hypothesis of equal average. 



### Linear regressions


##### With sklearn


To get the actual regression 
```python
from sklearn.linear_model import LinearRegression

X = df[chosen_features].to_numpy()
y = df[to_predict]

model = LinearRegression()
model.fit(X, y)

print(model.score(X, y)) ## R_square score
print(model.intercept_) ## intercept coeff
print(model.coef_) ## coef of the regression line
```
Sklearn also features a lot of different feature calculator like `sklearn.metrics.mean_squared_error(labels_ground_truth, predicted)`.


Do the cross_validation with **precision** and **recall**.

```python
precision = cross_val_score(model, X, y, cv=10, scoring="precision")
recall = cross_val_score(model, X, y, cv=10, scoring="recall")
```


##### With SMF
There is two ways to do. One works with `statsmodel.formula.api` and works with formula for the time 
> formula = "`var ~ C(predictor1) + C(predictor2) + ... + C(predictork) + predictori:predictorj`"

- `:`: interactaction term between predictori and predictorj i.e. a term of the form $\alpha$* predictori* predictorj
- `+`: Union of term included in the model
- `*`: `a*b`= `a + b + a:b` 
- `C(var)`: to specify categorical variables


Now having the statistics of this regression yields :

```python
import statsmodel.formula.api as smf

model = smf.ols(formula="time ~ C(diabetes) + C(high_blood_pressure)", data=df)
res = model.fit() ## fits the model according to the given data

print(res.summary())
```

The summary looks like this : 
```
                            OLS Regression Results                            
==============================================================================
Dep. Variable:                   time   R-squared:                       0.040
Model:                            OLS   Adj. R-squared:                  0.033
Method:                 Least Squares   F-statistic:                     6.097
Date:                Thu, 13 Jan 2022   Prob (F-statistic):            0.00254
Time:                        15:00:11   Log-Likelihood:                -1718.9
No. Observations:                 299   AIC:                             3444.
Df Residuals:                     296   BIC:                             3455.
Df Model:                           2                                         
Covariance Type:            nonrobust                                         
===============================================================================================
                                  coef    std err          t      P>|t|      [0.025      0.975]
-----------------------------------------------------------------------------------------------
Intercept                     139.3851      6.658     20.934      0.000     126.282     152.489
C(diabetes)[T.1]                4.9059      8.949      0.548      0.584     -12.706      22.518
C(high_blood_pressure)[T.1]   -31.8228      9.247     -3.441      0.001     -50.021     -13.624
==============================================================================
Omnibus:                      159.508   Durbin-Watson:                   0.076
Prob(Omnibus):                  0.000   Jarque-Bera (JB):               18.166
Skew:                           0.076   Prob(JB):                     0.000114
Kurtosis:                       1.802   Cond. No.                         2.82
==============================================================================
```

- First thing : **the R square score** : 0.040 is low
- you got values for all of the coefficient

Here we can deduce that time = 139 + 4 * diabetes - 31.82 * high_blood_pressure, where both are binary indicators.

OLS = Ordinary least square but you can perform the same analysis with `smf.logit` to have a logistic regression


### Creating a matching in an observational study


We need to match two persons, one from the control group and the other from the treated one. We need to mathc two persons that have the closest **propensity score** as possible. That is, two persons from the two groups that seems to have, apparently, the same probability to be treated.

##### Computing the propensity score

First, do a linear regression of **all our features*** with **treated**.

```python
## what we try to do is estimate the relation between treat and all the other features
import statsmodels.formula.api as smf

model = smf.logit(formula="treat ~ age + educ + re74 + re78 + re75 + C(black) + C(hispan) + C(married) + C(nodegree)", data=df)
res = model.fit()
print(res.summary())
df["propensity_score"] = res.predict() ## this is the propensity score of each data point
```


##### Matching the points

We want to match the datapoints

```python
import networkx as nx

control = df[df.treat == 0]
treated = df[df.treat == 1]

weighted_edge_list = []
for controled_id, controled_features in control.iterrows():
    for treated_id, treated_features in treated.iterrows():
        edge_weight = 1 - np.abs(controled_features["propensity_score"] - treated_features["propensity_score"])
        weighted_edge_list.append((controled_id, treated_id, edge_weight))

G = nx.Graph()
G.add_weighted_edges_from(weighted_edge_list)

matching = nx.max_weight_matching(G)
```



### Get tthe ROC curve

It's pretty cumbursome to compute so you can just use

```python
from sklearn.metrics import roc_curve

fp_rate, tp_rate, _ = roc_curve(ground_truth_label, prediction)

plt.plot(fp_rate, tp_rate)
```

